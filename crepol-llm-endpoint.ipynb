{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5875d4a6-c874-4b29-b5ad-79df9997341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "## Servicio LLM (Flask)\n",
    "## Este servicio se encargará de ejecutar el modelo LLM y generar respuestas.\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_name = \"pablomo83/Llama-3-credpolF\"\n",
    "model_file = \"Llama-3-credpol-unsloth.Q8_0.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
    "## model_path = \"S:\\\\Users\\\\Pablo\\\\Source\\\\Repos\\\\llm-rscore-pol\\\\kuak1\\\\models\\\\pablomo83\\\\Llama-3-credpol\\\\unsloth.Q8_0.gguf\"\n",
    "model_path = \"S:\\\\Users\\\\Pablo\\\\Source\\\\Repos\\\\llm-rscore-pol\\\\kuak1\\\\models\\\\pablomo83\\\\Llama-3-credpol\\\\Llama-3-credpol-unsloth.Q4_K_M.gguf\"\n",
    "\n",
    "\n",
    "stop_token = \"\\n\"\n",
    "LLM_SERVICE_PORT=5001\n",
    "\n",
    "## Instantiate model from downloaded file\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,  # Context length to use\n",
    "    n_batch= 512,\n",
    "    n_threads=4,            # Number of CPU threads to use\n",
    "    n_gpu_layers=32,        # Number of model layers to offload to GPU\n",
    "    input_prefix=\"[INST]\",\n",
    "    input_suffix= \"[/INST]\\\\n\",\n",
    "    antiprompt= [\n",
    "      \"[INST]\"\n",
    "    ],\n",
    "    pre_prompt= \"Tu eres un asistente de politicas de creditos en Kuak S.A.\",\n",
    "    pre_prompt_suffix= \"<</SYS>>[/INST]\\\\n\",\n",
    "    pre_prompt_prefix= \"[INST]<<SYS>>\\\\n\"\n",
    ")\n",
    "\n",
    "## Generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":1024,\n",
    "    \"stop\":[\"\\n\"], \n",
    "    \"echo\":True, # Echo the prompt in the output\n",
    "    \"top_k\":1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb2ab9-05c7-4f59-9435-04d51c9ad6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['MAX_CONTENT_LENGTH'] = 4096\n",
    "app.config['JSON_AS_ASCII'] = False\n",
    "\n",
    "log = logging.getLogger('werkzeug')\n",
    "log.setLevel(logging.ERROR)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Endpoint de mensajeria'\n",
    "\n",
    "# Manejador de errores global\n",
    "@app.errorhandler(Exception)\n",
    "def handle_exception(e):\n",
    "    logging.error(\"Error general: %s\", e)\n",
    "    print(\"Error\")\n",
    "    traceback.print_exc()\n",
    "    return \"Ha ocurrido un error en el servidor.\", 500\n",
    "\n",
    "@app.route(\"/ping\", methods=['GET'])\n",
    "def ping():\n",
    "    return \"El servicio de LLM está vivo!\"\n",
    "\n",
    "@app.route(\"/generate\", methods=['POST'])\n",
    "def generate_response():\n",
    "    try:\n",
    "        data = request.json\n",
    "        incoming_msg = data.get(\"prompt\", \"\").strip()\n",
    "        prompt = f'Eres un ChatBot asistente de Kuak S.A., una firma multinacional de servicios de auditoría, consultoría y advisory con 80000 empleados alrededor del mundo. Esta compañía imaginaría implementa las unidades de cariño como un sistema complementario de recompensas y bonificaciones. De manera estándar, la compañía otorga a sus empleados 3 unidades de cariño por hora a empleados que prestan servicios en proyectos facturables a terceros, 2 unidades de cariño por hora a quienes prestan servicios en proyectos internos de valor agregado o funciones directivas y una unidad de cariño por cada hora trabajo en tareas administrativas o no facturables. Los jefes también pueden otorgar una cantidad acotadas de unidades de cariño como forma de recompensa para promover la innovación de la práctica en servicios calves o estratégicos. Además, Kuak, implementa una novedosa política de crédito donde los empleados pueden pedir unidades de cariño a crédito, dependiendo el destino tendrán un interés variable. Si el destino de las unidades de cariño es de uso personal, como ser, intercambiar las unidades de cariño por horas libres, adelanto de sueldo, vacaciones extra entonces tendran interés. Si en cambio tiene un destino profesional que tambien beneficia a la compañía, como por ejemplo becas en el sistema educativo de cada país, certificaciones profesionales, acceso a curso, entonces serán libre de interés. A continuación te haran una pregunta y tu daras tu mejor Respuesta.\\n ### Pregunta: {incoming_msg}\\n ### Respuesta:'\n",
    "        \n",
    "        if prompt:\n",
    "            response = llm(prompt, **generation_kwargs)  ## llama_cpp.generate(model, prompt=incoming_msg) ## llm(prompt, **generation_kwargs)\n",
    "            # Extraer el texto generado del JSON\n",
    "            generated_text = response['choices'][0]['text']\n",
    "            response_clean = generated_text.replace(prompt, '').strip()\n",
    "            return jsonify({\"response\": response_clean})\n",
    "        else:\n",
    "            return jsonify({\"response\": \"No se recibió un prompt válido.\"}), 400\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error en /generate: %s\", e)\n",
    "        print(\"Error\")\n",
    "        traceback.print_exc()  # Imprimir el traceback completo en la terminal\n",
    "        return jsonify({\"response\": \"Ha ocurrido un error al generar la respuesta.\"}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=LLM_SERVICE_PORT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6180cc-195f-424d-b3f9-3a156f1810a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
